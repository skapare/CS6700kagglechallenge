{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import dateutil\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data in basic table form\n",
    "df = pd.read_csv(\"./train_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time_to_attribution'] = df.apply(lambda row: datetime.strptime(row['attributed_time'], '%Y-%m-%d %H:%M:%S') - datetime.strptime(row['click_time'], '%Y-%m-%d %H:%M:%S') if isinstance(row['attributed_time'], str) else '', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['click_time_dt'] = df.apply(lambda row: (dateutil.parser.parse(row['click_time'], dayfirst=True) - datetime.utcfromtimestamp(0)).total_seconds() , axis=1)\n",
    "\n",
    "ip = df.groupby('ip')['ip']\n",
    "min_time = df.groupby('ip')[\"click_time_dt\"].min()\n",
    "max_time = df.groupby('ip')[\"click_time_dt\"].max()\n",
    "num_clicks = df.groupby('ip')[\"click_time_dt\"].count()\n",
    "\n",
    "freq = num_clicks / (max_time - min_time)\n",
    "import math\n",
    "f_list = []\n",
    "for f in freq:\n",
    "    if not math.isinf(f):\n",
    "        f_list.append(f)\n",
    "freq2 = sum(f_list)/len(f_list)\n",
    "period = 1/freq2\n",
    "arr = np.array([f_list])\n",
    "stdev = np.std(arr)\n",
    "outliers = arr[(arr - np.mean(arr)) > 2 * np.std(arr)]\n",
    "np.max(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fv_labels(df):\n",
    "    labels = df['is_attributed']\n",
    "    fv = df.drop(['click_time', 'attributed_time', 'time_to_attribution','is_attributed'], axis=1)\n",
    "    return fv, labels\n",
    "\n",
    "def scale_fv(mat):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    return np.transpose(min_max_scaler.fit_transform(np.transpose(mat)))\n",
    "df = df.sample(frac=1)\n",
    "df_train = df.head(60000)\n",
    "df_test = df.tail(40000)\n",
    "\n",
    "train_fv, train_labels = get_fv_labels(df_train)\n",
    "test_fv, test_labels = get_fv_labels(df_test)\n",
    "\n",
    "train_fv_np = train_fv.as_matrix()\n",
    "test_fv_np = test_fv.as_matrix()\n",
    "\n",
    "train_labels_np = train_labels.as_matrix()\n",
    "test_labels_np = test_labels.as_matrix()\n",
    "\n",
    "train_fv_np_sc = scale_fv(train_fv_np)\n",
    "test_fv_np_sc = scale_fv(test_fv_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00225"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#probability of finding a 1 from lables\n",
    "prob_1 = np.count_nonzero(train_labels_np)/len(train_labels)\n",
    "prob_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 0.0017650127410888672\n",
      "score 0.995125\n",
      "predict [0 0 0 ... 0 0 0]\n",
      "labels [0 0 0 ... 0 0 0]\n",
      "sum 92\n",
      "incorrect 185\n",
      "incorrect_ratio 0.004625\n",
      "dummy_prediction_result 0.995375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "def dummy_classifier(fv, labels):\n",
    "    clf = sklearn.dummy.DummyClassifier(strategy='stratified', random_state=None)\n",
    "    clf.fit(fv, labels) \n",
    "    return clf\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "clf = dummy_classifier(train_fv_np_sc,train_labels_np) \n",
    "\n",
    "end = time.time()\n",
    "print('time',end - start)\n",
    "\n",
    "#print(timeit.timeit(svm(test_mat_scaled)))\n",
    "predict = clf.predict(test_fv_np_sc)\n",
    "labels = test_labels_np\n",
    "\n",
    "\n",
    "\n",
    "print('score',clf.score(test_fv_np_sc,labels))\n",
    "\n",
    "print('predict',predict)\n",
    "print('labels',labels)\n",
    "\n",
    "print('sum',sum(labels))\n",
    "\n",
    "\n",
    "incorrect = np.sum(np.bitwise_xor(predict,labels))\n",
    "\n",
    "\n",
    "print('incorrect',incorrect)\n",
    "\n",
    "print('incorrect_ratio',(incorrect/40000))\n",
    "\n",
    "print('dummy_prediction_result',(1-(incorrect/40000)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 0.04658102989196777\n",
      "score 0.9977\n",
      "predict [0 0 0 ... 0 0 0]\n",
      "labels [0 0 0 ... 0 0 0]\n",
      "sum 92\n",
      "incorrect 92\n",
      "incorrect_ratio 0.0023\n",
      "lr_prediction_result 0.9977\n"
     ]
    }
   ],
   "source": [
    "def lr(fv, labels):\n",
    "    # change solver to sag or saga for large dataset, and include max_iter = 100\n",
    "    clf = sklearn.linear_model.LogisticRegression(penalty='l1', dual=False, tol=0.001, C=1, solver='liblinear')\n",
    "    clf.fit(fv, labels) \n",
    "    return clf\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "clf = lr(train_fv_np_sc,train_labels_np) \n",
    "\n",
    "end = time.time()\n",
    "print('time',end - start)\n",
    "\n",
    "#print(timeit.timeit(svm(test_mat_scaled)))\n",
    "predict = clf.predict(test_fv_np_sc)\n",
    "labels = test_labels_np\n",
    "\n",
    "\n",
    "\n",
    "print('score',clf.score(test_fv_np_sc,labels))\n",
    "\n",
    "print('predict',predict)\n",
    "print('labels',labels)\n",
    "\n",
    "print('sum',sum(labels))\n",
    "\n",
    "\n",
    "incorrect = np.sum(np.bitwise_xor(predict,labels))\n",
    "\n",
    "\n",
    "print('incorrect',incorrect)\n",
    "\n",
    "print('incorrect_ratio',(incorrect/40000))\n",
    "\n",
    "print('lr_prediction_result',(1-(incorrect/40000)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 34857 unique IPs.\n",
      "There are 161 unique Apps.\n",
      "There are 100 unique Devices.\n",
      "There are 130 unique OSs.\n",
      "There are 161 unique Channels.\n"
     ]
    }
   ],
   "source": [
    "#let's have a look at the data\n",
    "#how many features do we have?\n",
    "#how many categories within the features?\n",
    "#can we get use this info to get a Gini score and start the CART tree?\n",
    "print('There are', len(df[\"ip\"].unique()), 'unique IPs.')\n",
    "print('There are', len(df[\"app\"].unique()), 'unique Apps.')\n",
    "print('There are', len(df[\"device\"].unique()), 'unique Devices.')\n",
    "print('There are', len(df[\"os\"].unique()), 'unique OSs.')\n",
    "print('There are', len(df[\"channel\"].unique()), 'unique Channels.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef MultiVarDT(fv, labels):\\n    # change solver to sag or saga for large dataset, and include max_iter = 100\\n    clf = DecisionTreeClassifier(criterion = \"gini\", random_state = 100, max_depth=3, min_samples_leaf=5)\\n    clf.fit(fv, labels) \\n    return clf\\n\\nimport time\\nstart = time.time()\\n\\nclf = MultiVarDT(train_fv_np_sc,train_labels_np) \\n\\nend = time.time()\\nprint(\\'time\\',end - start)\\n\\n#print(timeit.timeit(svm(test_mat_scaled)))\\npredict = clf.predict(test_fv_np_sc)\\nlabels = test_labels_np\\n\\n\\n\\nprint(\\'score\\',clf.score(test_fv_np_sc,labels))\\n\\nprint(\\'predict\\',predict)\\nprint(\\'labels\\',labels)\\n\\nprint(\\'number of 1s\\',sum(labels))\\n\\n\\nincorrect = np.sum(np.bitwise_xor(predict,labels))\\n\\n\\nprint(\\'incorrect\\',incorrect)\\n\\nprint(\\'incorrect_ratio\\',(incorrect/40000))\\n\\nprint(\\'MultiClassDT_prediction_result\\',(1-(incorrect/40000)))\\n'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def MultiVarDT(fv, labels):\n",
    "    # change solver to sag or saga for large dataset, and include max_iter = 100\n",
    "    clf = DecisionTreeClassifier(criterion = \"gini\", random_state = 100, max_depth=3, min_samples_leaf=5)\n",
    "    clf.fit(fv, labels) \n",
    "    return clf\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "clf = MultiVarDT(train_fv_np_sc,train_labels_np) \n",
    "\n",
    "end = time.time()\n",
    "print('time',end - start)\n",
    "\n",
    "#print(timeit.timeit(svm(test_mat_scaled)))\n",
    "predict = clf.predict(test_fv_np_sc)\n",
    "labels = test_labels_np\n",
    "\n",
    "\n",
    "\n",
    "print('score',clf.score(test_fv_np_sc,labels))\n",
    "\n",
    "print('predict',predict)\n",
    "print('labels',labels)\n",
    "\n",
    "print('number of 1s',sum(labels))\n",
    "\n",
    "\n",
    "incorrect = np.sum(np.bitwise_xor(predict,labels))\n",
    "\n",
    "\n",
    "print('incorrect',incorrect)\n",
    "\n",
    "print('incorrect_ratio',(incorrect/40000))\n",
    "\n",
    "print('MultiClassDT_prediction_result',(1-(incorrect/40000)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 60000)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def MultiClassDecisionTree(fv,labels)\n",
    "    for fv\n",
    "\n",
    "train_fv_np_sc,train_labels_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
